<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art of Data Reduction: How PCA Makes Sense of Large Datasets | Shubham Goswami, PhD </title> <meta name="author" content="Shubham Goswami, PhD"> <meta name="description" content="Personal and professional website of Shubham Goswami, PhD. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://goswami-13.github.io/posts/2024/05/blog-post-19/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Art of Data Reduction: How PCA Makes Sense of Large Datasets",
            "description": "",
            "published": "May 21, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shubham</span> Goswami, PhD </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Guides </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Art of Data Reduction: How PCA Makes Sense of Large Datasets</h1> <p></p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> </nav> </d-contents> <p>The world around us may seem random and unpredictable, but even in chaos, patterns tend to emerge. This is also true for large datasets. Principal Component Analysis (PCA) is a powerful tool that can help us uncover these hidden patterns.</p> <figure> <img src="https://goswami-13.github.io/images/Post19/BlogLogo.jpeg" width="80%"> </figure> <p>If you’ve been following my previous articles, you’re likely familiar with the power of Singular Value Decomposition (SVD). We’ve explored its versatility in various applications, from dimensionality reduction to extracting valuable insights from complex datasets. As a cornerstone of data-driven techniques, SVD has proven indispensable in the realms of machine learning and data science, enabling tasks such as pattern recognition and classification with remarkable efficacy.</p> <p>Find my previous articles on <a href="https://medium.com/gitconnected/from-swirling-fluids-to-hidden-patterns-unveiling-insights-with-singular-value-decomposition-3284af808ed0" rel="external nofollow noopener" target="_blank">Singular Value Decomposition</a> and its <a href="https://medium.com/gitconnected/unlocking-insights-exploring-singular-value-decomposition-svd-and-its-dynamic-applications-238377ccbd51" rel="external nofollow noopener" target="_blank">Applications</a>.</p> <p>However, our journey into data exploration doesn’t end here. Today, we’re embarking on a deeper exploration into Principal Component Analysis (PCA). Whether you’re immersed in the intricacies of fluid dynamics or captivated by the possibilities of machine learning, PCA holds a special allure. In the realm of fluid dynamics, it’s often recognized under the more familiar guise of Proper Orthogonal Decomposition (POD).</p> <p>So, without delay, let’s delve into the transformative world of PCA!</p> <h2 id="pca--overview">PCA : Overview</h2> <p>Principal Component Analysis (PCA) stands as a pivotal application of Singular Value Decomposition (SVD), offering a data-driven and hierarchical perspective on high-dimensional correlated datasets. Unpacking this statement reveals a wealth of insights. Firstly, PCA relies on SVD as its fundamental framework, underlining its inherently data-centric nature. Secondly, the hierarchical representation it provides involves extracting dominant patterns from complex datasets and arranging them in order of significance, with SVD serving as a facilitator in this process. Lastly, the term “correlated dataset” warrants further exploration.</p> <p>In statistical terms, correlations serve as a vital tool for establishing relationships between variables within a dataset or even across multiple datasets. Specifically within the context of SVD and eigenvalue decomposition, correlations typically arise within a single dataset. If you’re already acquainted with Singular Value Decomposition, you’ll likely recognize the following matrices:</p> <figure> <img src="https://goswami-13.github.io/images/Post19/AATATA.png" width="10%"> </figure> <p>These are known as the correlation matrices. In linear algebra, these matrices hold a special significance. Let’s consider a matrix A with dimensions <code class="language-plaintext highlighter-rouge">m×n</code>. While it could be a square matrix, in many real-world scenarios, it’s often rectangular, taking the form of a tall-skinny matrix, as depicted below:</p> <figure> <img src="https://goswami-13.github.io/images/Post19/TallSkinny.png" width="10%"> </figure> <p>Regardless of its size, the correlation matrices derived from A will exhibit symmetry, squareness, and positive semi-definiteness (eigenvalues ≥ 0). Additionally, both matrices will share the same positive eigenvalues and rank as A, earning them the moniker <em>“covariance matrix.”</em> In <a href="https://medium.com/gitconnected/from-swirling-fluids-to-hidden-patterns-unveiling-insights-with-singular-value-decomposition-3284af808ed0" rel="external nofollow noopener" target="_blank">Singular Value Decomposition (SVD)</a>, the eigenvectors of $AA^T$ are represented by U, and those of $A^T A$ by V. In the context of SVD, we refer to them as singular values. These matrices’ positive eigenvalues, the square roots of which are singular values, remain identical.</p> <p>Returning to the topic of correlation matrices, we encounter two primary types: row-wise and column-wise. The matrix $AA^T$ represents the row-wise correlation matrix, computed by taking the inner product of A’s rows. Conversely, $A^T A$ signifies the column-wise correlation matrix, derived from the inner product of A’s columns. Let’s visualize these matrices to grasp the scale of correlations we’re about to handle.</p> <figure> <img src="https://goswami-13.github.io/images/Post19/Columnwise.PNG" width="80%"> </figure> <figure> <img src="https://goswami-13.github.io/images/Post19/Rowwise.PNG" width="80%"> </figure> <p>Comparing the sizes of these matrices, let’s consider a time-series dataset represented by matrix A. Here, columns denote “Snapshots,” while rows encapsulate the data flattened into individual columns. Consequently, A takes on dimensions <code class="language-plaintext highlighter-rouge">m×n</code>, rendering $AA^T$ as <code class="language-plaintext highlighter-rouge">m×m</code> and $A^T A$ as <code class="language-plaintext highlighter-rouge">n×n</code>.</p> <p>Constructing the row-wise correlation matrix $AA^T$, especially for large <code class="language-plaintext highlighter-rouge">m</code>, proves impractical in many applications due to its considerable dimensions and associated computational challenges. For instance, in fluid dynamics simulations or experiments with a million degrees of freedom at each time instance (where m=1 million), $AA^T$ would comprise one trillion elements. While techniques like snapshot methods can mitigate this, they’re beyond our current scope. Hence, in practice, we often compute the eigenvalue decomposition of $A^T A$, which is more manageable in size.</p> <p><strong>This matrix, $A^TA$, encapsulates the correlation pivotal to Principal Component Analysis (PCA) discussions.</strong></p> <h2 id="pca--the-math">PCA : The Math</h2> <p>Let’s embark on our exploration using the tall-skinny matrix A, which represents a time-series dataset. Our journey begins with computing the row-wise mean, effectively averaging each row of A:</p> \[\overline{a_{j}} = \frac{1}{m} \sum_{i=1}^m A_{ij}\] <p>This operation yields the mean matrix:</p> <figure> <img src="https://goswami-13.github.io/images/Post19/MeanMatrix.png" width="10%"> </figure> <p>Subsequently, we derive a mean-subtracted matrix (B) by subtracting the mean matrix from A:</p> \[B = A - \overline{A}\] <p>Utilizing this Mean-subtracted matrix B, we construct a correlation matrix C using column-wise correlation:</p> \[C = \frac{1}{m-1} B^T B\] <p>Now, let’s delve into two approaches for identifying principal components. Firstly, by employing the mean-subtracted matrix B, one can ascertain principal components through its Singular Value Decomposition (SVD):</p> \[B = USV^T\] <p>In this scenario, the first principal component emerges as:</p> \[u_1 = U_{(0,0)}S_0V^T_{(0,0)}\] <p>Alternatively, we can obtain principal components from the correlation matrix C by computing its eigenvalue decomposition:</p> \[CV = VD\] <p>Here, V represents the eigenvectors, and D signifies the eigenvalues of C.</p> <p>Lets consider some example to better understand PCA.</p> <h2 id="example--multivariate-gaussian-distribution">Example : Multivariate Gaussian Distribution</h2> <p>Let’s explore a simple example of a multivariate Gaussian distribution, akin to the dataset showcased on the Wikipedia page for PCA. We’ll generate this dataset comprising 10,000 data points drawn from a two-dimensional normal distribution centered at (1, 3) with standard deviations of (3, 1) along the x and y directions, respectively. Subsequently, we’ll rotate this cloud of data points by <code class="language-plaintext highlighter-rouge">π/6</code> radians.</p> <p>Firstly, let’s generate the dataset, and then we’ll delve into the various analyses facilitated by PCA. To follow along, fire up a Jupyter notebook and import the necessary dependencies.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### Import necessary modules
</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">colors</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">'</span><span class="s">font.size</span><span class="sh">'</span> <span class="p">:</span> <span class="mi">22</span><span class="p">,</span> <span class="sh">'</span><span class="s">font.family</span><span class="sh">'</span> <span class="p">:</span> <span class="sh">'</span><span class="s">Times New Roman</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">text.usetex</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">})</span>
</code></pre></div></div> <p>Generate the data set,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xC</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>      <span class="c1"># Center of data (mean)
</span><span class="n">sig</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>   <span class="c1"># Principal axes
</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">6</span>           <span class="c1"># Rotate cloud by pi/3
</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)],</span>     <span class="c1"># Rotation matrix
</span>              <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)]])</span>

<span class="n">nPoints</span> <span class="o">=</span> <span class="mi">10000</span>            <span class="c1"># Create 10,000 points
</span>
<span class="c1">### Data
</span><span class="n">X</span> <span class="o">=</span> <span class="n">R</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">sig</span><span class="p">)</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">nPoints</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">xC</span><span class="p">)</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="n">nPoints</span><span class="p">))</span>
</code></pre></div></div> <p>Plot using matplotlib,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="sh">''</span><span class="p">,</span> <span class="n">markerfacecolor</span> <span class="o">=</span> <span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">markeredgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Data</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$x$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$y$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_aspect</span><span class="p">(</span><span class="sh">'</span><span class="s">equal</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <img src="https://goswami-13.github.io/images/Post19/Data.jpeg" width="50%"> </figure> <p>Now, let’s utilize this dataset to perform <strong>Principal Component Analysis (PCA) and extract the first three principal components along with their confidence intervals.</strong> We’ll begin by constructing the mean-subtracted matrix and then proceed with a straightforward eigenvalue decomposition. Follow the script below to execute this task:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Xavg</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>                  <span class="c1"># Compute mean
</span><span class="n">B</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">Xavg</span><span class="p">,(</span><span class="n">nPoints</span><span class="p">,</span><span class="mi">1</span><span class="p">)).</span><span class="n">T</span>       <span class="c1"># Mean-subtracted data
</span>
<span class="c1"># Find principal components (SVD)
</span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">svd</span><span class="p">(</span><span class="n">B</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">nPoints</span><span class="p">),</span><span class="n">full_matrices</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>Then find the first confidence interval by,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># 1-std confidence interval
</span><span class="n">Xstd</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)</span> <span class="o">@</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)])</span>
</code></pre></div></div> <p>To find the first three confidence intervals, one can multiply the interval by 1, 2, and 3. Additionally, plotting the principal components of this data cloud can be achieved using the left singular matrix (U) and the largest eigenvalues (S). Below is a script to execute these tasks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="sh">''</span><span class="p">,</span> <span class="n">markerfacecolor</span> <span class="o">=</span> <span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">markeredgecolor</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Data</span><span class="sh">'</span><span class="p">)</span>

<span class="c1">### Plot confidence intervals
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">Xavg</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">Xstd</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">Xavg</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">Xstd</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">Xavg</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">Xstd</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">Xavg</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">Xstd</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">Xavg</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">Xstd</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">Xavg</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">Xstd</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot principal components
</span><span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">Xavg</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xavg</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span> 
        <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">Xavg</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Xavg</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">U</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">cyan</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">Xavg</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Xavg</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">U</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">S</span><span class="p">[</span><span class="mi">1</span><span class="p">]]),</span> 
        <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">Xavg</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Xavg</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">U</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">S</span><span class="p">[</span><span class="mi">1</span><span class="p">]]),</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">cyan</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$x$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$y$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">set_aspect</span><span class="p">(</span><span class="sh">'</span><span class="s">equal</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <img src="https://goswami-13.github.io/images/Post19/Intervals.jpeg" width="50%"> </figure> <h2 id="example--brest-cancer-data">Example : Brest Cancer Data</h2> <p>Let’s apply Principal Component Analysis (PCA) to a more complex dataset, such as Scikit-learn’s Breast Cancer Dataset. This dataset comprises breast cancer data from 569 female patients, with each observation containing 30 attributes. The high dimensionality of this dataset presents several challenges. To begin, we’ll confirm the level of correlation within the data by employing Singular Value Decomposition (SVD) and examining the variance captured by the first few modes.</p> <p>To follow along, fire up a Jupyter notebook and proceed with the following steps. Load the dependencies:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">colors</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">'</span><span class="s">font.size</span><span class="sh">'</span> <span class="p">:</span> <span class="mi">22</span><span class="p">,</span> <span class="sh">'</span><span class="s">font.family</span><span class="sh">'</span> <span class="p">:</span> <span class="sh">'</span><span class="s">Times New Roman</span><span class="sh">'</span><span class="p">,</span> <span class="sh">"</span><span class="s">text.usetex</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">})</span>
</code></pre></div></div> <p>Load the breast cancer dataset from Scikit-learn:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="nf">load_breast_cancer</span><span class="p">()</span>
<span class="n">cancer</span><span class="p">.</span><span class="nf">keys</span><span class="p">()</span>
<span class="c1">### Output
### dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])
</span></code></pre></div></div> <p>Create a Dataframe with this new dataset:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">cancer</span><span class="p">[</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">],</span><span class="n">columns</span><span class="o">=</span><span class="n">cancer</span><span class="p">[</span><span class="sh">'</span><span class="s">feature_names</span><span class="sh">'</span><span class="p">])</span>
<span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div> <figure> <img src="https://goswami-13.github.io/images/Post19/CancerData.PNG" width="90%"> </figure> <p>At this point, apply Singular Value Decomposition (SVD) to assess the level of correlation. The cumulative explained variance ratio plot will help us understand how much variance is captured by the first few principal components.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">vt</span> <span class="o">=</span> <span class="n">LA</span><span class="p">.</span><span class="nf">svd</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="c1">### SVD
</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">semilogy</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">markerfacecolor</span> <span class="o">=</span> <span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">markeredgecolor</span> <span class="o">=</span> <span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$n$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Singular Values,</span><span class="sh">'</span> <span class="o">+</span> <span class="sa">r</span><span class="sh">'</span><span class="s">$\sigma_n$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">cumulative</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>  <span class="c1">### Cumulative spectrum
</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">cumulative</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">'</span><span class="s">o</span><span class="sh">'</span><span class="p">,</span> <span class="n">markerfacecolor</span> <span class="o">=</span> <span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">markeredgecolor</span> <span class="o">=</span> <span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$n$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$\sum\sigma_n$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_tick_params</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_ticks_position</span><span class="p">(</span><span class="sh">'</span><span class="s">both</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p><img src="https://goswami-13.github.io/images/Post19/SingularValues.jpeg" width="50%"><img src="https://goswami-13.github.io/images/Post19/Cumulative.jpeg" width="50%"></p> <p>The plots indeed reveal that a significant amount of variance is captured by the first few modes of this data. This observation suggests a high level of correlation among the features of breast cancer patients, indicating substantial overlap in their cancer characteristics. <strong>The ability to visualize such patterns and correlations in a high-dimensional dataset underscores the importance of Principal Component Analysis (PCA).</strong></p> <p>Returning to the Breast Cancer Dataset, with its 30 dimensions, visualizing it directly becomes challenging. Therefore, we’ll employ PCA to extract the first three principal components and visualize them in a 3D space. Let’s conduct this PCA analysis using scikit-learn:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### Scalar fitting step for normalization
</span><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">scaled_data</span> <span class="o">=</span><span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1">### Perform PCA
</span><span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">pca</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">)</span>
<span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1">### Dataset with reduced dimensionality
</span><span class="n">x_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1">### original data shape
</span><span class="nf">print</span><span class="p">(</span><span class="n">x_pca</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1">### new data shape
</span>
<span class="c1">### Output
### (569, 30)
### (569, 3)
</span></code></pre></div></div> <p>Plot this data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">projection</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">3d</span><span class="sh">'</span><span class="p">},</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_pca</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">x_pca</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_pca</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">cancer</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">],</span><span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">prism</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$PC1$</span><span class="sh">'</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$PC2$</span><span class="sh">'</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_zlabel</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">$PC3$</span><span class="sh">'</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>

<span class="n">ax</span><span class="p">.</span><span class="nf">view_init</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_box_aspect</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">zoom</span><span class="o">=</span><span class="mf">0.85</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <figure> <img src="https://goswami-13.github.io/images/Post19/CancerDataPlot.jpeg" width="50%"> </figure> <p>This image displays the Breast Cancer Dataset classified into the two specified targets: benign and malignant. Each point in the 3D space represents a patient, with their attributes projected onto the first three principal components extracted through PCA. The color gradient distinguishes between benign and malignant cases, offering insight into potential clusters and patterns within the dataset.</p> <h2 id="pca--take-aways">PCA : Take-aways</h2> <p>Principal Component Analysis (PCA) offers several key takeaways:</p> <ol> <li> <strong>Dimensionality Reduction:</strong> PCA enables the reduction of the dimensionality of data while preserving the most critical information. It accomplishes this by transforming the original variables into a new set of uncorrelated variables called principal components.</li> <li> <strong>Data Visualization:</strong> PCA aids in visualizing high-dimensional data by projecting it onto a lower-dimensional subspace while retaining as much variance as possible. This visualization simplifies the interpretation of complex datasets and facilitates exploratory data analysis.</li> <li> <strong>Feature Extraction:</strong> PCA identifies the most significant features in the data by ranking the importance of each principal component. This feature extraction process can help in identifying patterns, trends, and relationships within the dataset.</li> <li> <strong>Noise Reduction:</strong> PCA can mitigate the effects of noise and redundant information present in the data by focusing on the principal components with the highest variance. This leads to a more efficient representation of the data, which can improve the performance of subsequent analysis or modeling tasks.</li> <li> <strong>Multicollinearity Handling:</strong> PCA addresses multicollinearity issues by transforming correlated variables into a new set of orthogonal variables. This transformation reduces the redundancy among variables, which can enhance the stability and accuracy of statistical models.</li> </ol> <p>In summary, PCA provides a powerful framework for reducing the dimensionality of data, visualizing complex datasets, extracting essential features, handling multicollinearity, and enhancing the interpretability and efficiency of various data analysis tasks.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2025/06/blog-post-54/">Tabulating 6-DOF Motion Kinematics in OpenFOAM using Python</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2025/06/blog-post-53/">Setting Up Overset Mesh Cases in OpenFOAM Made Easy</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2025/06/blog-post-52/">An Introduction to Overset Mesh in OpenFOAM</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2025/05/blog-post-51/">Python Tools for Analyzing and Visualizing Mesh Motion Simulations</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2025/03/blog-post-50/">Simulating Pure Pitching and Heaving Motions in OpenFOAM</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Shubham Goswami, PhD. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>